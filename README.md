# ğŸ§  CNN Animations


Animations that explain the building blocks of **Convolutional Neural Networks (CNNs)**, step by step.


<p align="center">
<img src="exports/gif/conv2d_valid.gif" width="520" alt="Convolution animation preview"/>
</p>


---


## ğŸ“Œ Why CNNs?


A **Convolutional Neural Network (CNN)** is a deep learning model designed for gridâ€‘like data such as images or audio spectrograms. CNNs:


- ğŸ” Use **local receptive fields** (kernels look at small patches)
- â™»ï¸ Apply **weight sharing** (the same kernel slides everywhere)
- ğŸ— Build a **hierarchy of features** (edges â†’ textures â†’ shapes â†’ objects)


---


## ğŸŒŠ What is a Feature Map?


After applying a kernel and an activation (e.g., ReLU), you get a **feature map**:


- Highlights where the kernelâ€™s pattern appears
- Different kernels â†’ different feature maps (edges, corners, textures)
- Stacking many maps builds rich image representations


---


## ğŸ¬ Episode 1 â€” Convolution


We start with the most fundamental operation: **2D Convolution**.


- Input: **5Ã—5 grid**
- Kernel: **3Ã—3 filter**
- Padding: **valid**
- Stride: **1**


At each step:
1. Multiply input Ã— kernel (elementwise)
2. Sum them up â†’ one output cell
3. Slide to the next position


This produces a **3Ã—3 feature map**, encoding the presence of the kernelâ€™s pattern across the input.


---


## ğŸ“– Want more details?


See the longâ€‘form explanation here: **[docs/intro.md](docs/intro.md)**


---


## ğŸ›£ Roadmap


- [x] Episode 1 â€” Convolution
- [ ] Episode 2 â€” Padding & Stride
- [ ] Episode 3 â€” Pooling (Max/Average)
- [ ] Episode 4 â€” Activations (ReLU, Sigmoid, â€¦)
- [ ] Episode 5 â€” Loss & Backpropagation overview


---


## ğŸ“œ License


MIT â€” free to use and share.